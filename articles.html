<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text x='50%' y='55%' dominant-baseline='middle' text-anchor='middle' font-size='60' font-family='Helvetica'>AP</text></svg>">
    <title>AP - Articles</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        body {
            font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
            font-weight: 400;
            background-color: #000000;
            color: #ffffff;
        }
        @media (max-width: 768px) {
            .container {
                padding-left: 16px;
                padding-right: 16px;
            }
        }
        .back-to-top {
            position: fixed;
            bottom: 20px;
            right: 20px;
            display: none;
            background-color: rgba(59, 130, 246, 0.5);
            color: white;
            padding: 10px;
            border-radius: 5px;
            cursor: pointer;
            transition: opacity 0.3s;
        }
        .back-to-top:hover {
            opacity: 1;
        }
        
        /* Add these new styles */
        html, body {
            overflow-x: hidden;
            width: 100%;
            position: relative;
        }
        
        @media (max-width: 768px) {
            .container {
                padding-left: 16px;
                padding-right: 16px;
                max-width: 100vw;
            }
        }
    </style>
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        sans: ['"Helvetica Neue"', 'Helvetica', 'Arial', 'sans-serif'],
                    },
                }
            }
        }
    </script>
    <link href="https://cdn.jsdelivr.net/npm/daisyui@3.7.3/dist/full.css" rel="stylesheet" type="text/css" />
</head>
<body class="font-sans leading-normal tracking-normal">
    <header class="text-center py-2">
        <h1 class="text-4xl font-normal mt-12 mb-4 text-white">Aryaman Pandya</h1>
    </header>

    <nav class="navbar bg-slate-800 opacity-70 text-white max-w-fit mx-auto rounded-xl mt-4 px-4 sm:px-12">
        <div class="flex flex-wrap justify-center">
            <a href="index.html" class="btn btn-ghost text-sm sm:text-lg normal-case font-light py-1 px-2 sm:px-4">About</a>
            <a href="resume.html" class="btn btn-ghost text-sm sm:text-lg normal-case font-light py-1 px-2 sm:px-4">Resume</a>
            <a href="articles.html" class="btn btn-ghost text-sm sm:text-lg normal-case font-light py-1 px-2 sm:px-4">Articles</a>
            <a href="bookshelf.html" class="btn btn-ghost text-sm sm:text-lg normal-case font-light py-1 px-2 sm:px-4">Bookshelf</a>
            <a href="quotes.html" class="btn btn-ghost text-sm sm:text-lg normal-case font-light py-1 px-2 sm:px-4">Quotes</a>
        </div>
    </nav>

    <div class="container mx-auto px-4 sm:px-8 md:px-16 lg:px-32 xl:px-48 py-8 mt-6 mb-8">
        <p class="text-lg font-light text-center text-gray-300 mb-12 max-w-2xl mx-auto">
            A slightly structured log of miscellaneous thoughts.
        </p>

        <div class="mb-16 bg-slate-800 rounded-xl p-6 max-w-lg mx-auto">
            <ul class="space-y-2 text-center">
                <li><a href="#hindsight" class="text-blue-400 hover:text-blue-300 transition-colors duration-200">2024</a></li>
                <li><a href="#o1" class="text-blue-400 hover:text-blue-300 transition-colors duration-200">O1</a></li>
            </ul>
        </div>

        <div id="hindsight" class="mb-12">
            <h3 class="text-3xl font-normal mb-4 text-white">2024 in hindsight</h3>
            <h3 class="text-xl font-normal mb-4 text-slate-400">December 27, 2024</h3>
            <br>
            <p>Unless you occasionally choose to operate at a significant proportion of the speed of light, the way you experience each year of your adult life should feel temporally alike. Despite this fundamental constraint, many claim that the rate at which you experience life “speeds up” after a certain age. Some claim that this has to do with a reduced frequency of novel events, while other work argues it has something to do with neurodegeneration and a reduced efficiency in visual perception. In supporting either one of those points of view, I would be well out of my depth.            </p>
            <br>
            <p>Regardless of the cause (or existence) of such phenomena, I thought it might be a good idea to write out a fresh recounting of the year I just experienced before my brain undoubtedly starts its process of confabulation. </p>
            <br>
            <h2 class="text-2xl font-semibold mb-4">Career</h2>
            <p>I started the year continuing to grow in my role at Motional, where I learned and built some interesting things. I received invaluable guidance on how to go from writing scrappy code with copious technical debt creation to being able to design reliable (reasonably) large scale systems that generate slightly less debt. I used this guidance to  architect and implement a multi-stage ML system that aimed to automate the operational/engineering processes of detecting and diagnosing failures across the autonomous vehicle fleet. Building something that processed tremendous amounts of multimodal data was a great learning experience.</p>
            <br>
            <p>On a related note, I was invited to be a reviewer at the ACM SIGCHI Conference on Automotive User Interfaces and Interactive Vehicular Applications (AutoUI) this year. This was a novel experience that I really enjoyed and learned a lot from - particularly about the current state of research in automotive human-machine interaction.</p>
            <br>
            <p>Eventually, my time at Motional was cut short due to <a href="https://techcrunch.com/2024/05/14/motional-cut-about-550-employees-around-40-in-recent-restructuring-sources-say/" class="text-blue-400 hover:text-blue-300 transition-colors duration-200">restructuring</a> at the company. Nonetheless, I'm grateful for what I learned there and the people I got to work with. Shortly after this, I started a new role at Intersystems Corporation in Cambridge, MA.</p>
            <br>
            <h2 class="text-2xl font-semibold mb-4">Programming</h2>
            <p>I spent a good amount of my “free time” building independent projects this year. With some of these, the focus was the process. I built things with the objective to learn the skills necessary to build them. Others were built with the intent to produce real value. I’ve listed some of these from each category below.            </p>
            <br>
            <h3 class="text-xl font-semibold mb-4">Educational</h3>
            <ul class="list-disc pl-5 mt-4 space-y-2 text-gray-300">
                <li><a href="https://github.com/apandy02/transformers" class="text-blue-400 hover:text-blue-300 transition-colors duration-200">Character-level GPT</a>
                    <ul class="list-disc pl-5 mt-2">
                        <li>I trained an autoregressive decoder only model on all the Harry Potter books and got it to generate somewhat context-coherent text. I built the transformer and all of its components "from scratch" in PyTorch.</li>
                    </ul>
                </li>
                <li><a href="https://github.com/apandy02/vision" class="text-blue-400 hover:text-blue-300 transition-colors duration-200">Diffusers</a>
                    <ul class="list-disc pl-5 mt-2">
                        <li>I revisited Computer Vision as a topic, with the goal of being able to build a diffusion model from scratch (thanks to a lot of help from Jeremy Howard and Fast AI)</li>
                        <li>In notebooks, I experimented first with smaller things (resnets for classification), extracted components from this code into a small "from scratch" (in PyTorch) vision library and worked my way up to implementing Denoising Diffusion Probabilistic (and Implicit) Models</li>
                    </ul>
                </li>
                <li> Vision applications
                    <ul class="list-disc pl-5 mt-2">
                        <li>I had experience building real-world deployed ML (albeit classical) applications, and I had now gained some new vision skills. To put these together, I built a few different things. Of note:</li>
                        <li>I built a <a href="https://github.com/apandy02/livedetection" class="text-blue-400 hover:text-blue-300 transition-colors duration-200">real time detection streaming application</a> that would detect faces in frames of a live stream and present them to a client. With this one, I learned about WebRTC and improved my understanding of asynchronous systems.</li>
                        <li>I also built a <a href="https://github.com/apandy02/video_processing_ml_service" class="text-blue-400 hover:text-blue-300 transition-colors duration-200">post-processing cloud service</a> that would take in video data (for example, video logs from a real world robot) and identify features from frames of the video. With this one I developed some architecture/infrastructure skills. I worked with Kubernetes and a suite of AWS services (EKS, Lambda, SQS message queues)</li>
                    </ul>
                </li>
            </ul>
            <br>
            <img src="images/github_contribs.png" alt="GitHub Contributions 2023 vs 2024" class="w-3/4 mx-auto mb-2 mt-4">
            <p class="text-base text-gray-400 text-center mb-4">GitHub contributions in 2024 compared to 2023. I'm aware this isn't an ideal metric to optimize for, but I'm happy there was an increase in output.</p>
            <br>
            <h3 class="text-xl font-semibold mb-4">Larger, more intentional projects</h3>
            <p>I am yet to "ship" any of the projects in this category, but these were developed (or are being developed) with the intention of producing real value. I'm going to follow up with more detailed articles individually describing these projects, but for the sake of documenting everything of significance I did this year in one place, I built:</p>
            <ul class="list-disc pl-5 mt-4 space-y-2 text-gray-300">
                <li>An educational companion for early educational support. Think tamagotchi in a 3D world, but powered with a speech to speech AI pipeline, and designed to help its user learn better. I have a lot to say about the utility, expressiveness, and design of this product, but I'll do so in a standalone article. In order to build this (along with a couple of friends) I taught myself some Unity, C#, and game design for this.</li>
                <li>An automated nutritionist/sports scientist. I built myself an application that centralizes all of my already digitized data (from my whoop, smart scale, etc), allows for LLM-based nutrition logging in natural language, and most importantly: provides me with science backed feedback on a daily, weekly, and monthly timescale. I built this using <a href="https://docs.ell.so" class="text-blue-400 hover:text-blue-300 transition-colors duration-200">ell</a> and <a href="https://fastht.ml" class="text-blue-400 hover:text-blue-300 transition-colors duration-200">FastHTML</a>, both of which were released this year. I look forward to sharing more about this one soon.</li>
            </ul>
            <br>
            <p>In addition to working on my own projects, I finally started contributing to a few open source projects this year (better late than never). I hope that this becomes a larger part of my life moving forward.            </p>
            <br>
            <h2 class="text-2xl font-semibold mb-4">Books</h2>
            <p>I managed to read a number of books that had been collecting dust in my bookshelf for a while. I won't list all of them here (you can see my <a href="/bookshelf.html" class="text-blue-400 hover:text-blue-300 transition-colors duration-200">bookshelf</a>), instead I wanted to capture a series of snippets (quotes, interpretations) that I would not like to lose. Below is a list of a few of these             </p>
            <br>
            <p class="text-lg font-semibold mb-2">The Idea Factory (John Gertner) and The Computer and The Brain (John Von Neumann)</p>
            <p>It's possible for a small contingency of the most brilliant people of a period to see the future. If you brought the average person (including the average scientist/engineer) who lived in the 1940s to 2024 and exposed them to a high fidelity augmented reality device or told them about the wonders of AI we've created, it quite possibly would lead to an anxiety attack. However, if you could bring John Von Neumann or Claude Shannon to the present, it would probably feel "just right" to them. They'd then probably find their bearings and shortly make their way to research labs where they'd immediately start contributing to the state of the art. How many Shannons and Von Neumann's do we have in the world today?</p>
            <br>
            <p class="text-lg font-semibold">The Bhagavad Gita</p>
            <ul class="list-disc pl-5 mt-4 space-y-2 text-gray-300">
                <li><em>You have the right to work, but for the work's sake only. You have no right to the fruits of work. Desire for the fruits of work must never be your motive in working. Never give way to laziness, either.</em></li>
                <li><em>Work done with anxiety about results is far inferior to work done without such anxiety, in the calm of self-surrender.</em></li>
            </ul>
            <br>
            <p class="text-lg font-semibold mb-2">Hackers and Painters (Paul Graham)</p>
            <p>Programming is more akin to an art than science. This is not to say there aren't fundamental aspects of programming that are deeply rooted in science, as a matter of fact a lot of the work I'm interested in is scientific. But outside of finding optimal solutions to constrained problems, the process of building a magical product is more artistic than it is scientific.</p>
            <br>
            <h2 class="text-2xl font-semibold mb-4">General Lessons</h2>
            <ul class="list-disc pl-5 mt-4 space-y-2 text-gray-300 mb-6">
                <li>When an opportunity presents itself, dedicate some time to understand your own biases and presumptions before assessing it. Then, give more time and thought than you originally anticipated.                 </li>
                <li>It's good to run a mental tree search on futures based on the outcomes of decisions you can make. Just be aware that certain branches might stimulate your mind more than others. Make sure you don't give these branches any extra attention because the actual process of simulating futures can take away from your ability to act in the present.</li>
                <li>We're animals of inertia. It takes considerable force to overcome this inertia. Find states of inertia that lead you to require minimal variance and then live in these states.</li>
                <li>If you really love what you do, do more of it. Then, do some more.</li>
                <li>Understand your internal reward model. You might want to apply a discount factor to actions based on the horizon of the rewards they lead to.</li>
            </ul>
            <h2 class="text-2xl font-semibold mb-4">What I look forward to doing next year</h2>
            <p>I'd like 2025 to be a year where I continue to develop along the same axes with greater compounding effects. Specifically, I'd like to be able to ship a few different paid products of my own and take some leaps entrepreneurially. I also want to continue to build things to learn things. A non-exhaustive list of things I'd like to learn more about are:</p>
            <ul class="list-disc pl-5 mt-4 space-y-2 text-gray-300">
                <li>GPU programming</li>
                <li>LLM Reasoners</li>
                <li>Interpretability</li>
                <li>Diffusion models for multiple modalities</li>
                <li>More efficient systems design and maintenance</li>
            </ul>
            <br>
            <p class="mb-6">It is my hope that this essay elicits higher accountability from me. When I write the 2025 version of this, it should feel like I continued growing along these axes at an increased rate. If this is not the case, I have not done myself justice.</p>
        </div>

        <!-- Article divider -->
        <hr class="border-t border-white opacity-20 my-12 max-w-4xl mx-auto">

        <div id="o1" class="mb-12">
            <h3 class="text-3xl font-normal mb-4 text-white">An O1 Inspired Survey: Modeling Chain of Thought Generation as a Reinforcement Learning Problem</h3>
            <h3 class="text-xl font-normal mb-4 text-slate-400">October 14, 2024</h3>
            <br>
            <p>In game-playing AI systems, pure neural networks perform considerably worse than versions of themself augmented with search. For example, AlphaZero <a href="#citation-1" style="color: #87CEEB;">[<span style="color: #87CEEB;">1</span>]</a> showed that the network without Monte-Carlo Tree Search (MCTS) was able to achieve a chess ELO rating of 2500. This is high, but not by any means superhuman. In contrast, the complete system with MCTS appended to the base model is able to achieve an ELO rating of 3500 which is ~600 points higher than the highest rating achieved by a human (<a href="https://en.wikipedia.org/wiki/Comparison_of_top_chess_players_throughout_history" style="color: #87CEEB;">Magnus Carlson</a>).</p>
            <br>
            <p>During training and at test time, MCTS is able to search through potential futures, evaluate the outcomes of making different decisions, and select ones that maximize expected reward. In addition to test-time search, AlphaZero is trained with self-play, a Reinforcement Learning technique that enables the system to simulate games by playing against previous versions of itself (if interested, see my implementation of AlphaZero <a href="https://github.com/aryamanpandya99/alphazero_nano" style="color: #87CEEB;">here</a>). This synthetically generated experience reduces the dependency of the system on human generated training data and encourages deeper exploration of the solution space. This deep exploration can lead to the discovery of novel solutions in situations that are rarely encountered (see <a href="https://www.wired.com/2016/03/two-moves-alphago-lee-sedol-redefined-future/" style="color: #87CEEB;">move 37, AlphaGo v/s Lee Sedol</a>).</p>
            <br>
            <p>Similarly, embedding search-based mechanisms within the larger system should help large transformer-based language models produce better answers to hard, reasoning dependent problems. Hypothetically, these methods would enable the same kind of evaluation of future outcomes, allowing models to carefully answer questions while assessing the vast solutions pace.</p>
            <br>
            <p>Recently, OpenAI introduced their "reasoning model", O1 <a href="#citation-2" style="color: #87CEEB;">[<span style="color: #87CEEB;">2</span>]</a>. O1 is said to have been trained using reinforcement learning and to "think" before answering questions. According to benchmark results reported by OpenAI, O1 indicates a step-function improvement to the capabilities of language models at solving difficult problems in domains like math, science and computer programming. Further, the publicly released O1-preview and O1-mini displayed significant improvements on the LMSY Chatbot Arena leaderboard, a crowdsourced platform that evaluates the performance of Large Language Models (LLMs) based on human preferences, thus proving its versatility in multiple domains. </p>
            <br>
            <p>As we've come to expect, OpenAI withheld information about the training process they use as well as their datasets. They go as far as to protect the chains of thought generated by models in response to prompts. Therefore, what we can know about O1 is limited to the information they released in their blog posts, system card, and speculation from the community based on existing literature.</p>
            <br>
            <p>I thought it would be a good exercise for me to review surrounding literature to try and interpret what OpenAI might be doing here, and that's what this "article" is going to be about.</p>
            <br>
            <p>The success of O1 comes as the culmination of success of three intuitions:</p>
            <br>
            <ol class="list-decimal pl-5 mt-4 space-y-2 text-gray-300">
                <li>Scaling the amount of compute available to the model at inference time leads to better answers to difficult questions</li>
                <li>Chain of thought prompting elicits reasoning in large language models <a href="#citation-3" style="color: #87CEEB;">[<span style="color: #87CEEB;">3</span>]</a>, and can be viewed as a policy improvement operator <a href="#citation-4" style="color: #87CEEB;">[<span style="color: #87CEEB;">4</span>]</a></li>
                <li>We can use reinforcement learning in conjunction with large reward models to build out a self-training process</li>
            </ol>
            <br>
            <p>In early 2022, OpenAI released Wei et al. 2022 <a href="#citation-3" style="color: #87CEEB;">[<span style="color: #87CEEB;">3</span>]</a>. This paper demonstrated that including few-shot examples of chains of thought within prompts to the model can significantly increase the accuracy of its outputs. Mechanistically, we know that "the model gathers answer tokens from the generated context, the question context, as well as the few-shot context" <a href="#citation-5" style="color: #87CEEB;">[<span style="color: #87CEEB;">5</span>]</a>. Few-shot CoT prompting has since been used as a reliable means of producing consistently higher quality outputs from language models across many domains. It is now standard practice to include samples that use CoT in datasets used to pre-train and fine-tune models.</p>
            <br>
            <p>While the description above would make it seem like this paper was the basis mainly for the first half of the second intuition listed above, I would argue that it fueled exploration along all of the other lines in that list. In this paper, the authors briefly mention that "additional compute can be allocated to more complex problems." While stated in passing (whether intentionally or not) this is a key insight we'll go into further detail on later on.</p>
            <br>
            <p>Shortly after the release of this paper, Zelikman et. al. released "STaR: Bootstrapping Reasoning with Reasoning" <a href="#citation-8" style="color: #87CEEB;">[<span style="color: #87CEEB;">8</span>]</a>. This paper introduces the STaR (Self-taught Reasoner) fine-tuning process that combines CoT prompting with outcome supervision to produce an iteratively improving model. STaR has a generally simple training process:</p>
            <br>
            <ol class="list-decimal pl-5 mt-4 text-gray-300">
                <li>Using few shot CoT, produce responses from an LLM given a set of prompts</li>
                <li>Using automation, check for correctness of the outputs</li>
                <li>Append correct solutions (including the CoT that led to them) to a dataset that can be used to fine tune the model</li>
                <li>In situations where the answers are incorrect, prompt the LLM the correct answer and have it reverse engineer a CoT that would have led to this output</li>
                <li>Include these in the dataset used for finetuning</li>
                <li>Iterate</li>
            </ol>
            <img src="images/star.png" alt="STaR Algorithm" class="w-3/4 mx-auto mb-2 mt-2">
            <p class="text-base text-gray-400 text-center">The formal STaR algorithm from the paper</p>
            <br>
            <p>This method builds on the results from the OpenAI paper that CoT improves model outputs, and builds a process that allows for the model to iteratively teach itself to develop better chains of thought. This led to significant improvements over the benchmark results of the base model. For
                example, on the Commonsense Question Answering (CQA) dataset, the base model (GPT-J) achieved an accuracy of 20.5%, a direct finetune on the entire CQA training set achieved a 60% accuracy, STaR without the rationalizing step achieved a 68.8% accuracy, and STaR with the rationalizing step achieved 72.5% accuracy.
                It's noteworthy, that the base model was trained on 0% of the CQA dataset, the direct finetune used 100% of the training dataset, star without rationalization used 86.7% of the training dataset, and STaR with rationalization used 86.7% of the training dataset.</p>
            <br>
            <p>This paper also briefly formulates this training objective as an approximation to the RL policy gradient objective. It doesn't, however, use this insight to guide the training process, or dive deeper into a mathematical analysis of this approximation. </p>
            <br>
            <p>STaR builds the intuition that more inference time computation can lead to better results, and also defines a self-play like training procedure. That being said, there were a few obvious shortcomings of this method:             </p>
            <br>
            <ul class="list-disc pl-5 mt-4 space-y-2 text-gray-300">
                <li>As long as the final answer is correct, a sample is included in the iteratively generated fine-tuning dataset. This will include outputs where flawed CoT’s mistakenly yielded correct answers.
                </li>
                <li>The authors try to avoid wasting data by generating reverse engineered CoTs, but these could produce an exaggerated version of the problem in bullet 1. There's no means in the methodology of this paper to verify that the forced CoTs contain coherent reasoning. </li>
                <li>This paper ignores the learning benefit that could come from samples where the model fails entirely (true negatives).</li>
            </ul>
            <br>
            <p>The first two limitations mentioned are due to the fact that this method uses outcome-based supervision instead of process-based supervision. In outcome-based supervision, we only judge the correctness of the output of the model. In process-based supervision, we judge the correctness of each step in a CoT.            </p>
            <br>
            <p>With these considerations in mind, in Lightman et al. 2023 <a href="#citation-6" style="color: #87CEEB;">[<span style="color: #87CEEB;">6</span>]</a>, researchers from OpenAI train a Process-supervised Reward Model (PRM) by fine-tuning a base model. The PRM is trained on the outputs of a "generator" language model where each step of the CoT is labeled for correctness. </p>
            <br>
            <p>In this work, the authors focus solely on training a PRM, and not on fine-tuning the generator model that the PRM is judging. They do, however, state that "fine-tuning with RL is a natural next step." (maybe there's some foreshadowing here) </p>
            <br>
            <p>Before kicking off the PRM training process, they fine-tune the generator to ensure that it produces CoTs in a consistent, line delimited format. They kick off the training with an initial batch in the following way:</p>
            <br>
            <ol class="list-decimal pl-5 mt-4 text-gray-300">
                <li>Collect generator outputs</li>
                <li>Get human-annotation on these outputs</li>
                <li>Train the PRM by fine-tuning a base model on this human annotation</li>
            </ol>
            <br>
            <p>Then, they begin the iterative training procedure:</p>

            <ol class="list-decimal pl-5 mt-4 text-gray-300">
                <li>Collect generator outputs</li>
                <li>Determine the "most convincing wrong answers" using the latest version of the PRM. These are answers that are determined to be wrong, but contained CoTs that were able to trick the PRM.</li>
                <li>Surface the "most convincing wrong answers" to human annotators for labeling</li>
                <li>Include this newly labeled data in the training set</li>
                <li>Re-train the PRM on this data</li>
                <li>Repeat steps 1-5 iteratively until some level of convergence</li>
            </ol>
            <br>
            <p>An important detail to note is that the PRM also evaluates the final answer and treats it like any other step in the CoT.</p>

            <p>As part of their evaluation procedure, the authors develop an ORM (Outcome-supervised Reward Model) in addition to the PRM to compare performance. They find that while both methods outperform the base model with few-shot CoT and majority voting, the PRM considerably outperforms the ORM on (78.2% accuracy v/s 72.4% accuracy). This is insightful — the coherence of the CoT is important in leading it to a correct answer. It further underlines the idea that a purely outcome based model (like STaR) have areas for improvement, and encourages further work to train verifiers for the process, not just the outcome. In my opinion, there's a strong possibility that this training procedure, and RL based generator improvements are two of the fundamental building blocks of O1.</p>
            <br>
            <p>The authors of the STaR paper subsequently released Zelikman et al. 2024 <a href="#citation-9" style="color: #87CEEB;">[<span style="color: #87CEEB;">9</span>]</a> detailing the "Quiet-STaR" algorithm. This paper acknowledges and builds off of the following shortcomings of STaR identified by the authors:</p>

            <ul class="list-disc pl-5 mt-4 space-y-2 text-gray-300">
                <li>They trained and evaluated STaR on very narrow domains and only on question-answer type samples.</li>
                <li>Training on labeled samples of CoT is expensive and also off-policy (the samples are not generated by the same model that is being trained).</li>
            </ul>
            <br>
            <p>To address these shortcomings, instead of training only on question-answer samples and basing the objective on the outcome, they formulate a non-myopic auto-regressive objective. What this means is that they supervise their training procedure on the semantic correctness of the next N tokens predicted based on a rationale. They train the model on a more vast corpus of text, specifically OpenWebMath and Colossal Clean Crawled Corpus (C4).</p>
            <br>
            <img src="images/quiet_star.png" alt="Quiet-STaR Parallel Generation" class="w-1/2 mx-auto mb-2 mt-2">
            <p class="text-base text-gray-400 text-center">Parallel generation process in Quiet-STaR algorithm</p>
            <br>
            <p>The Quiet-STaR paper takes the idea of scaling inference-time compute further than the previously discussed algorithms. At each token in the input sequence, the model generates multiple rationales of length T to aid in the prediction of the next tokens in the sequence. This process obviously creates orders of magnitude of increased computational complexity which the authors reduce by generating these tokens in parallel for n tokens in the input sequence using an efficiently cached attention mask. helps visualize this process, and the open source code <insert link> details how this was implemented.</p>
            <br>
            <p>The objective of the training procedure is the sum of two separate loss functions. The first of these is the negative log likelihood loss of the predicted next-N tokens. The second of these is a formulation of the REINFORCE loss function. In order to construct the REINFORCE loss, they define the reward function as the difference between <span class="math inline">\(p^{talk}\)</span> and <span class="math inline">\(p^{-talk}\)</span> where <span class="math inline">\(p^{talk}\)</span> is the selected rationale's output distribution and <span class="math inline">\(p^{-talk}\)</span> is the average across rationales for that token. </p>
            <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
            <br>
            <p>
                This reward function mentioned above is represented by the following equation:
            </p>
            <br>
            <img src="images/qstarreward.png" alt="Quiet-STaR Reward Function" class="w-3/4 mx-auto mb-2 mt-2">
            <p class="text-base text-gray-400 text-center mb-4">Reward function used in the Quiet-STaR algorithm</p>
            <p>This training procedure leads to improvements in the quality of outputs, with some notable improvements against the baseline being 8.1% v/s 5.9% on the GSM8K dataset, and 42.6% v/s 36.3% on the CQA dataset.</p>
            <br>
            <p>While these numbers are measurable improvements, they are far less impressive when normalizing this improvement by the increased computational complexity. This procedure also raises a few questions surrounding its practicality, a few of which were raised by the authors in the paper:</p>
            <br>
            <ul class="list-disc pl-5 mt-4 space-y-2 text-gray-300">
                <li>The authors note that the thought tokens (and therefore added inference time compute) helped with certain types of text, but with other domains led to no measurable improvements. This makes us question whether the added expenditure on test-time compute is worth it.</li>
                <li>They note that even within the same text, not all tokens benefit from having thought tokens leading up to them. This may lead to a waste of compute and realistically would be impractical to deploy in any production system</li>
            </ul>
            <br>
            <p>It's unlikely O1 is doing something like this on every single token, but I thought the formulation of the reinforce loss was an interesting way of incorporating RL and was worth noting.</p>
            <br>
            <p>Several other papers propose approaches along the lines of STaR. For example, Hosseini et al. 2024 <a href="#citation-10" style="color: #87CEEB;">[<span style="color: #87CEEB;">10</span>]</a> trains a verifier during the STaR training process to learn from wrong answers too. Instead of creating reverse engineered CoTs in the case of wrong answers, they use both the wrong and correct solutions to train a verifier using Direct Preference Optimization (DPO). At inference time, this verifier is used as a judge to select an answer from K proposals. </p>
            <br>
            <p>Most of the methods discussed so far are Verifier-based techniques to scale test-time compute. Ie, they use an auxiliary model that is trained to verify proposals sampled from a generator model. In August 2024, Google posted Snell et al. 2024 <a href="#citation-11" style="color: #87CEEB;">[<span style="color: #87CEEB;">11</span>]</a> that examines a number of verifier and non-verifier based methods of test-time compute scaling. Interestingly, they designed an experiment to study the effectiveness of different methods to scale test-time compute given a constant compute budget. This was a recent publication and details many different methods, so I won't go into too much detail on any one of them, but I will highlight a few things that stood out to me when I read through it.            </p>
            <br>
            <p>On a high level:</p>
            <ul class="list-disc pl-5 mt-4 space-y-2 text-gray-300">
                <li>The benefits of scaling methods vary across different types of problems.</li>
                <li>Scaling inference time compute helps more on more difficult problems as long as the base LLM contains the knowledge required to come up with an answer. The most challenging types of questions still benefit from additional pre-training data/compute. This result is loosely illustrated in the figure below.</li>
            </ul>
            <br>
            <img src="images/difficulty.png" alt="Relationship between problem difficulty and scaling methods" class="w-3/4 mx-auto mb-2 mt-2">
            <p class="text-base text-gray-400 text-center mb-4">Effectiveness of scaling methods across problem difficulty levels</p>
            <br>
            <p>Instead of using human annotation to train a PRM (like PRM800k), based on Wang et al. <a href="#citation-7" style="color: #87CEEB;">[<span style="color: #87CEEB;">7</span>]</a> they train and supervise one using estimates of per-step correctness obtained from running Monte Carlo rollouts from each step in the solution.</p>
            <br>
            <p>Below, I briefly describe the different methods they examined, and the results they each obtain.</p>
            <h3 class="text-xl font-semibold mt-4 mb-2">Verifier-based methods</h3>
            <img src="images/prm_search.png" alt="PRM Search Methods Comparison" class="w-3/4 mx-auto mb-4 mt-6">
            <p class="text-base text-gray-400 text-center mb-2">Different PRM-based search methods</p>
            <br>
            <ul class="list-disc pl-5 mt-2 space-y-2 text-gray-300">
                <li><strong>Beam search:</strong> explores multiple paths simultaneously, keeping the top K most promising candidates at each step and stepping from them at the next. It balances exploration and exploitation by maintaining a diverse set of high-scoring partial solutions. It performs better than all other verifier based methods on harder problems. You can think of beam search as lookahead search with K = 0 (this will make sense in one moment).</li>
                <li><strong>Lookahead search:</strong> extends beam search by performing rollouts of varying depths (K) to evaluate the potential of each candidate with the PRM scoring the final step of the rollout. It allows for deeper exploration of promising paths. This is more like the MCTS rollouts in AlphaZero, with the only difference being that the PRM is frozen here. This method blows up computationally, and there's a limit to how much they could scale in these experiments. It doesn't seem to outperform other methods as much as you'd think (at least based on these experiments).</li>
                <li><strong>Best of N weighted:</strong> This approach generates multiple solutions and groups them based on their final answers. The scores of solutions leading to the same correct answer are summed, and the answer with the highest total score is selected. This one's surprisingly good, and it keeps scaling well. Instead of a traditional best of N where we pick the highest rated final answer, they group together all solutions that lead to the same final answer, even if the steps or reasoning are different. Instead of just picking the single highest-scoring solution, they add up all the individual scores for the steps leading to each right answer. The final answer with the highest total score wins.</li>
                <li><strong>Majority voting:</strong> multiple solutions are generated, and the most common final answer is selected as the output. Performance hits a wall as the budget grows.</li>
            </ul>
            <br>
            <img src="images/verifier_results.png" alt="Verifier-based methods performance on MATH dataset" class="w-1/2 mx-auto mb-4 mt-4">
            <p class="text-base text-gray-400 text-center mb-2">Performance of verifier-based methods on MATH dataset vs. inference compute budget</p>
            <br>
            <h3 class="text-xl font-semibold mt-4 mb-2">Refining Proposal Distribution</h3>
            <p>Using the principle of self-revision, this approach allows language models to correct mistakes made in previous reasoning traces. They follow the training procedure from Qu et al. 2024 <a href="#citation-12" style="color: #87CEEB;">[<span style="color: #87CEEB;">12</span>]</a>. The main idea is that they formulate the training process as a multi-turn Markov Decision Process where at each turn the LLM can generate new tokens as well as edit tokens generated at previous steps. They divide compute to both sequential and parallel sampling to try and benefit from the local properties of sequential generation and the global properties of parallel sampling. This procedure is considerably different than the others discussed here, so I'll leave it for another time to explore its nuances. They find that on the MATH dataset this procedure can out-perform the search based methods at high compute budgets.</p>
            <br>
            <h3 class="text-xl font-semibold mt-4 mb-2">General comments:</h3>
            <p>It sort of feels like everyone, regardless of what specific approach they propose, believe in the three intuitions stated initially. Specifically, my takeaways are that inference-time compute scaling unlocks the ability for the model to interpolate better and training procedures for inference scaled models will have some sort of RL flavor to them. If these scaling laws stand the test of time, this would have broad implications across the stack. Inference compute will be more valuable than it previously was: it's typically easier to run model inference on a wide variety of GPUs (not just the top of the line NViDIA ones) which might (slightly) shake up the competitiveness of chip manufacturers. From a research perspective, resources will be split between methods to better scale pertaining and methods to better scale inference compute- even though the latter aims to scale test-time compute, experimentation and fine-tuning that leads to these types of systems will require training compute.</p>
            <br>
            <p>If there's one thing that the recent history of deep learning has taught me is that on largely general tasks, more simply crafted objectives produce astonishingly good solutions. I would expect that a similar scenario might play out with scaling inference time compute.</p>
            <br>
            <h3 class="text-xl font-semibold mt-4 mb-2">Bibliography</h3>
            <ol class="list-decimal pl-5 mt-4 space-y-2 text-gray-300">
                <li id="citation-1">Silver, David, et al. "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." Science 362.6419 (2018): 1140-1144.</li>
                <li id="citation-2">OpenAI. "O1 System Card." OpenAI, 12 Sept. 2024, openai.com/system-cards/o1.</li>
                <li id="citation-3">Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837.</li>
                <li id="citation-4">Zhang, Hugh, and David C. Parkes. "Chain-of-thought reasoning is a policy improvement operator." arXiv preprint arXiv:2309.08589 (2023).</li>
                <li id="citation-5">Dutta, Subhabrata, et al. "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning." arXiv preprint arXiv:2402.18312 (2024).</li>
                <li id="citation-6">Lightman, Hunter, et al. "Let's verify step by step." arXiv preprint arXiv:2305.20050 (2023).</li>
                <li id="citation-7">Wang, P., et al. "Math-shepherd: Verify and reinforce llms step-by-step without human annotations." arXiv preprint arXiv:2308.13785 (2023).</li>
                <li id="citation-8">Zelikman, Eric, et al. "Star: Bootstrapping reasoning with reasoning." Advances in Neural Information Processing Systems 35 (2022): 15476-15488.</li>
                <li id="citation-9">Zelikman, Eric, et al. "Quiet-star: Language models can teach themselves to think before speaking." arXiv preprint arXiv:2403.09629 (2024).</li>
                <li id="citation-10">Hosseini, Arian, et al. "V-star: Training verifiers for self-taught reasoners." arXiv preprint arXiv:2402.06457 (2024).</li>   
                <li id="citation-11">Snell, Charlie, et al. "Scaling llm test-time compute optimally can be more effective than scaling model parameters." arXiv preprint arXiv:2408.03314 (2024).</li>
                <li id="citation-12">Qu, Yuxiao, et al. "Recursive introspection: Teaching language model agents how to self-improve." arXiv preprint arXiv:2407.18219 (2024).</li>
            </ol>
        </div>
    </div>

    <footer class="bg-gray-900 py-4">
    </footer>

    <button id="backToTop" class="back-to-top">Back to Top</button>

    <script>
        // Back to top button
        const backToTopButton = document.getElementById('backToTop');
        window.addEventListener('scroll', () => {
            if (window.pageYOffset > 300) {
                backToTopButton.style.display = 'block';
            } else {
                backToTopButton.style.display = 'none';
            }
        });

        backToTopButton.addEventListener('click', () => {
            window.scrollTo({top: 0, behavior: 'smooth'});
        });
    </script>
</body>
</html>
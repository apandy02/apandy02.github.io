<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aryaman Pandya - Thoughts</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        body {
            font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
            font-weight: 400;
            background-color: #000000;
            color: #ffffff;
        }
        @media (max-width: 768px) {
            .container {
                padding-left: 16px;
                padding-right: 16px;
            }
        }
    </style>
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        sans: ['"Helvetica Neue"', 'Helvetica', 'Arial', 'sans-serif'],
                    },
                }
            }
        }
    </script>
    <link href="https://cdn.jsdelivr.net/npm/daisyui@3.7.3/dist/full.css" rel="stylesheet" type="text/css" />
</head>
<body class="font-sans leading-normal tracking-normal">
    <header class="text-center py-2">
        <h1 class="text-4xl font-normal mt-12 mb-4 text-white">Aryaman Pandya</h1>
    </header>

    <nav class="navbar bg-slate-800 opacity-70 text-white max-w-fit mx-auto rounded-xl mt-4 px-12">
        <div class="flex justify-center">
            <a href="index.html" class="btn btn-ghost text-lg normal-case font-light py-1">About</a>
            <a href="resume.html" class="btn btn-ghost text-lg normal-case font-light py-1">Resume</a>
            <a href="thoughts.html" class="btn btn-ghost text-lg normal-case font-light py-1">Thoughts</a>
        </div>
    </nav>

    <div class="container mx-auto px-4 sm:px-8 md:px-16 lg:px-32 xl:px-48 py-8 mt-6 mb-8">
        <p class="text-lg font-light text-center text-gray-300 mb-12 max-w-2xl mx-auto">
            An unstructured log of miscellaneous thoughts.
        </p>

        <div class="mb-12 bg-slate-800 rounded-xl p-6 max-w-lg mx-auto">
            <ul class="space-y-2 text-center">
                <li><a href="#thought1" class="text-blue-400 hover:text-blue-300 transition-colors duration-200">Quotes</a></li>
                <li><a href="#bookshelf" class="text-blue-400 hover:text-blue-300 transition-colors duration-200">Bookshelf</a></li>
                <li><a href="#thought2" class="text-blue-400 hover:text-blue-300 transition-colors duration-200">O1</a></li>
            </ul>
        </div>

        <div id="thought1" class="mb-12">
            <h3 class="text-2xl font-normal mb-4 text-white">Quotes</h3>
            <p class="text-gray-300">
               The following is a running list of quotes I found powerful when I came across them in books, podcasts, or other sources.
            </p>
            <ul class="list-disc pl-5 mt-4 space-y-4 text-gray-300">
                <li>
                    <p>"You can't know everything, but you should convince yourself you can know anything."</p>
                    <p class="text-sm text-gray-400 mt-1">- John Carmack</p>
                </li>
                <li>
                    <p>"We see an exponential curve but it's not because one thing is exponential, it's because we have hundreds of little sigmoids overlapped on each other that makes the general pattern look exponential"</p>
                    <p class="text-sm text-gray-400 mt-1">- John Carmack</p>
                </li>
                <li>
                    <p>"Up to a point it is better to let the snags [bugs] be there than to spend such time in design that there are none (how many decades would this course take?)."</p>
                    <p class="text-sm text-gray-400 mt-1">- Alan Turing</p>
                </li>
                <li>
                    <p>“Self-education is, I firmly believe, the only kind of education there is.”</p>
                    <p class="text-sm text-gray-400 mt-1">- Isaac Asimov</p>
                </li>
                <li>
                    <p>"After thousands of years of compounding scientific discovery and technological progress, we have figured out how to melt sand, add some impurities, arrange it with astonishing precision at extraordinarily tiny scale into computer chips, run energy through it, and end up with systems capable of creating increasingly capable artificial intelligence."</p>
                    <p class="text-sm text-gray-400 mt-1">- Sam Altman</p>
                </li>
                <li>
                    <p>"I wish I was weirder"</p>
                    <p class="text-sm text-gray-400 mt-1">- George Hotz</p>
                </li>
            </ul>
        </div>

        <div id="bookshelf" class="mb-12">
            <h3 class="text-2xl font-normal mb-4 text-white">Bookshelf</h3>
            <p class="text-gray-300 mb-4">
                A list of books on my bookshelf.
            </p>
            <ul class="list-disc pl-5 mt-4 space-y-4 text-gray-300">
                <li>
                    <p class="font-semibold">A Brief History of Time</p>
                    <p class="text-sm text-gray-400 mt-1">by Stephen Hawking</p>
                </li>
                <li>
                    <p class="font-semibold">Brief Answers to the Big Questions</p>
                    <p class="text-sm text-gray-400 mt-1">by Stephen Hawking</p>
                </li>
                <li>
                    <p class="font-semibold">QED: The Strange Theory of Light and Matter</p>
                    <p class="text-sm text-gray-400 mt-1">by Richard Feynman</p>
                </li>
                <li>
                    <p class="font-semibold">Six Not So Easy Pieces</p>
                    <p class="text-sm text-gray-400 mt-1">by Richard Feynman</p>
                </li>
                <li>
                    <p class="font-semibold">A New Kind of Science</p>
                    <p class="text-sm text-gray-400 mt-1">by Stephen Wolfram</p>
                </li>
                <li>
                    <p class="font-semibold">The Computer and the Brain</p>
                    <p class="text-sm text-gray-400 mt-1">by John Von Neumann</p>
                </li>
                <li>
                    <p class="font-semibold">The Annotated Turing</p>
                    <p class="text-sm text-gray-400 mt-1">by Charles Petzold</p>
                </li>
                <li>
                    <p class="font-semibold">The Art of Computer Programming Vol. 1: Fundamental Algorithms</p>
                    <p class="text-sm text-gray-400 mt-1">by Donald Knuth</p>
                </li>
                <li>
                    <p class="font-semibold">The Art of Computer Programming Vol. 2: Seminumerical Algorithms</p>
                    <p class="text-sm text-gray-400 mt-1">by Donald Knuth</p>
                </li>
                <li>
                    <p class="font-semibold">Centralized and Distributed Operating Systems</p>
                    <p class="text-sm text-gray-400 mt-1">by Gary J. Nutt</p>
                </li>
                <li>
                    <p class="font-semibold">Design Patterns: Elements of Reusable Object-Oriented Software</p>
                    <p class="text-sm text-gray-400 mt-1">by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides</p>
                </li>
                <li>
                    <p class="font-semibold">Clean Code: A Handbook of Agile Software Craftsmanship</p>
                    <p class="text-sm text-gray-400 mt-1">by Robert C. Martin</p>
                </li>
                <li>
                    <p class="font-semibold">Deep Learning with Python</p>
                    <p class="text-sm text-gray-400 mt-1">by François Chollet</p>
                </li>
                <li>
                    <p class="font-semibold">Building Machine Learning Powered Applications</p>
                    <p class="text-sm text-gray-400 mt-1">by Emmanuel Ameisein</p>
                </li>
                <li>
                    <p class="font-semibold">Probabilistic Robotics</p>
                    <p class="text-sm text-gray-400 mt-1">by Sebastian Thrun, Wolfram Burgard, and Dieter Fox</p>
                </li>
                <li>
                    <p class="font-semibold">Cracking the Coding Interview</p>
                    <p class="text-sm text-gray-400 mt-1">by Gayle Laakmann McDowell</p>
                </li>
                <li>
                    <p class="font-semibold">Hackers and Painters</p>
                    <p class="text-sm text-gray-400 mt-1">by Paul Graham</p>
                </li>
                <li>
                    <p class="font-semibold">Zero to One</p>
                    <p class="text-sm text-gray-400 mt-1">by Peter Thiel</p>
                </li>
                <li>
                    <p class="font-semibold">The Innovator's Dilemma</p>
                    <p class="text-sm text-gray-400 mt-1">by Clayton Christensen</p>
                </li>
                <li>
                    <p class="font-semibold">Obviously Awesome: How to Nail Product Positioning</p>
                    <p class="text-sm text-gray-400 mt-1">by April Dunford</p>
                </li>
                <li>
                    <p class="font-semibold">The General Theory of Employment, Interest and Money</p>
                    <p class="text-sm text-gray-400 mt-1">by J.M. Keynes</p>
                </li>
                <li>
                    <p class="font-semibold">Jobs</p>
                    <p class="text-sm text-gray-400 mt-1">by Walter Isaacson</p>
                </li>
                <li>
                    <p class="font-semibold">Elon Musk</p>
                    <p class="text-sm text-gray-400 mt-1">by Walter Isaacson</p>
                </li>
                <li>
                    <p class="font-semibold">The Idea Factory: Bell Labs and the Great Age of American Innovation</p>
                    <p class="text-sm text-gray-400 mt-1">by Jon Gertner</p>
                </li>
                <li>
                    <p class="font-semibold">Rafa</p>
                    <p class="text-sm text-gray-400 mt-1">by Rafael Nadal</p>
                </li>
                <li>
                    <p class="font-semibold">Open</p>
                    <p class="text-sm text-gray-400 mt-1">by Andre Agassi</p>
                </li>
                <li>
                    <p class="font-semibold">Meditations</p>
                    <p class="text-sm text-gray-400 mt-1">by Marcus Aurelius</p>
                </li>
                <li>
                    <p class="font-semibold">The Singularity Is Near</p>
                    <p class="text-sm text-gray-400 mt-1">by Ray Kurzweil</p>
                </li>
                <li>
                    <p class="font-semibold">Map and Territory</p>
                    <p class="text-sm text-gray-400 mt-1">by Eliezer Yudkowsky</p>
                </li>
                <li>
                    <p class="font-semibold">The Emotion Machine</p>
                    <p class="text-sm text-gray-400 mt-1">by Marvin Minsky</p>
                </li>
                <li>
                    <p class="font-semibold">The Society of Mind</p>
                    <p class="text-sm text-gray-400 mt-1">by Marvin Minsky</p>
                </li>
            </ul>
        </div>
    
        <div id="thought2" class="mb-12">
            <h3 class="text-3xl font-normal mb-4 text-white">O1: Modeling Chain of Thought Generation as a Reinforcement Learning Problem</h3>
            <h3 class="text-xl font-normal mb-4 text-slate-400">October 14, 2024</h3>
            <br>
            <p>In game-playing AI systems, pure neural networks perform considerably worse than versions of themself augmented with search. For example, AlphaZero <a href="#citation-1" style="color: #87CEEB;">[<span style="color: #87CEEB;">1</span>]</a> showed that the network without Monte-Carlo Tree Search (MCTS) was able to achieve an chess ELO rating of 2500. This is high, but not by any means superhuman. In contrast, the complete system with MCTS appended to the base model is able to achieve an ELO rating of 3500 which is ~600 points higher than the highest rating achieved by a human (Magnus Carlson).                 .</p>
            <br>
            <p>During training and at test time, MCTS is able to search through potential futures, evaluate the outcomes of making different decisions, and select ones that maximize expected reward. In addition to test-time search, AlphaZero is trained with self-play, a Reinforcement Learning technique that enables the system to simulate games by playing against previous versions of itself. This synthetically generated experience reduces the dependency of the system on human generated training data and encourages deeper exploration of the solution space. This deep exploration can lead to the discovery of novel solutions in situations that are rarely encountered (see move 37, AlphaGo v/s Lee Sedol).</p>
            <br>
            <p>Similarly, embedding search-based mechanisms within the larger system should help large transformer-based language models produce better answers to hard, reasoning dependent problems. Hypothetically, these methods would enable the same kind of evaluation of future outcomes, allowing models to carefully answer questions while assessing the vast solutions pace.</p>
            <br>
            <p>Recently, OpenAI introduced the their "reasoning model", O1. O1 is said to have been trained using reinforcement learning and to "think" before answering questions. According to benchmark results reported by OpenAI, O1 indicates a step-function improvement to the capabilities of language models at solving difficult problems in domains like math, science and computer programming. Further, the publicly released O1-preview and O1-mini display step function improvements on the LMSYS leaderboard, which for the last year has primarily seen incremental improvements to its max score, thus proving its versatility in multiple domains. As we've come to expect, OpenAI withheld information about the training process they use as well as their datasets. They go as far as to protect the chains of thought generated by models in response to prompts. Therefore, what we can know about O1 is limited to the information they released in their blog posts, system card, and speculation from the community based on existing literature.</p>
            <br>
            <p>I thought it would be a good exercise for me to review surrounding literature to try and interpret what OpenAI might be doing here, and that's what this "article" is going to be about.</p>
            <br>
            <p>The success of O1 comes as the culmination of success of three intuitions:</p>
            <br>
            <ol class="list-decimal pl-5 mt-4 space-y-2 text-gray-300">
                <li>Scaling the amount of compute available to the model at inference time leads to better answers to difficult questions</li>
                <li>Chain of thought prompting elicits reasoning in large language models <a href="#citation-2" style="color: #87CEEB;">[<span style="color: #87CEEB;">2</span>]</a>, and can be viewed as a policy improvement operator <a href="#citation-3" style="color: #87CEEB;">[<span style="color: #87CEEB;">3</span>]</a></li>
                <li>We can use reinforcement learning in conjunction with large reward models to build out a self-training process</li>
            </ol>
            <br>
            <p>In early 2022, OpenAI released Wei et al. 2022 <a href="#citation-2" style="color: #87CEEB;">[<span style="color: #87CEEB;">2</span>]</a>. This paper demonstrated that including few-shot examples of chains of thought within prompts to the model can significantly increase the accuracy of its outputs. Mechanistically, we know that "the model gathers answer tokens from the generated context, the question context, as well as the few-shot context" <a href="#citation-4" style="color: #87CEEB;">[<span style="color: #87CEEB;">4</span>]</a>. Few-shot CoT prompting has since been used as a reliable means of producing consistently higher quality outputs from language models across many domains. It is now standard practice to include samples that use CoT in datasets used to pre-train and fine-tune models.</p>
            <br>
            <p>While the description above would make it seem like this paper was the basis mainly for the first half of the second intuition listed above, I would argue that it fueled exploration along all of the other lines in that list. In this paper, the authors briefly mention that "additional compute can be allocated to more complex problems." While stated in passing (whether intentionally or not) this is a key insight we'll go into further detail on later on.</p>
            <br>
            <p>Shortly after the release of this paper, Zelikman et. al. released "STaR: Bootstrapping Reasoning with Reasoning" <a href="#citation-7" style="color: #87CEEB;">[<span style="color: #87CEEB;">7</span>]</a>. This paper introduces the STaR (Self-taught Reasoner) fine-tuning process that combines CoT prompting with outcome supervision to produce an iteratively improving model. STaR has a generally simple training process:</p>
            <br>
            <ol class="list-decimal pl-5 mt-4 text-gray-300">
                <li>Using few shot CoT, produce responses from an LLM given a set of prompts</li>
                <li>Using automation, check for correctness of the outputs</li>
                <li>Append correct solutions (including the CoT that led to them) to a dataset that can be used to fine tune the model</li>
                <li>In situations where the answers are incorrect, prompt the LLM the correct answer and have it reverse engineer a CoT that would have led to this output</li>
                <li>Include these in the dataset used for finetuning</li>
                <li>Iterate</li>
            </ol>
            <img src="images/star.png" alt="STaR Algorithm" class="w-3/4 mx-auto mb-2 mt-2">
            <p class="text-base text-gray-400 text-center">The formal STaR algorithm from the paper</p>
            <br>
            <p>This method builds on the results from the OpenAI paper that CoT improves model outputs, and builds a process that allows for the model to iteratively teach itself to develop better chains of thought. This led to significant improvements over the benchmark results of the base model. For
                example, on the Commonsense Question Answering (CQA) dataset, the base model (GPT-J) achieved an accuracy of 20.5%, a direct finetune on the entire CQA training set achieved a 60% accuracy, STaR without the rationalizing step achieved a 68.8% accuracy, and STaR with the rationalizing step achieved 72.5% accuracy.
                It's noteworthy, that the base model was trained on 0% of the CQA dataset, the direct finetune used 100% of the training dataset, star without rationalization used 86.7% of the training dataset, and STaR with rationalization used 86.7% of the training dataset.</p>
            <br>
            <p>This paper also briefly formulates this training objective as an approximation to the RL policy gradient objective. It doesn't, however, use this insight to guide the training process, or dive deeper into a mathematical analysis of this approximation. </p>
            <br>
            <p>STaR builds the intuition that more inference time computation can lead to better results, and also defines a self-play like training procedure. That being said, there were a few obvious shortcomings of this method:             </p>
            <br>
            <ul class="list-disc pl-5 mt-4 space-y-2 text-gray-300">
                <li>As long as the final answer is correct, a sample is included in the iteratively generated fine-tuning dataset. This will include outputs where flawed CoT’s mistakenly yielded correct answers.
                </li>
                <li>The authors try to avoid wasting data by generating reverse engineered CoTs, but these could produce an exaggerated version of the problem in bullet 1. There's no means in the methodology of this paper to verify that the forced CoTs contain coherent reasoning. </li>
                <li>This paper ignores the learning benefit that could come from samples where the model fails entirely (true negatives).</li>
            </ul>
            <br>
            <p>The first two limitations mentioned are due to the fact that this method uses outcome-based supervision instead of process-based supervision. In outcome-based supervision, we only judge the correctness of the output of the model. In process-based supervision, we judge the correctness of each step in a CoT.            </p>
            <br>
            <p>With these considerations in mind, in Lightman et al. 2023 <a href="#citation-5" style="color: #87CEEB;">[<span style="color: #87CEEB;">5</span>]</a>, researchers from OpenAI train a Process-supervised Reward Model (PRM) by fine-tuning a base model. The PRM is trained on the outputs of a "generator" language model where each step of the CoT is labeled for correctness. </p>
            <br>
            <p>In this work, the authors focus solely on training a PRM, and not on fine-tuning the generator model that the PRM is judging. They do, however, state that "fine-tuning with RL is a natural next step." (Possibly foreshadowing O1?) </p>
            <br>
            <p>Before kicking off the PRM training process, they fine-tune the generator to ensure that it produces CoTs in a consistent, line delimited format. They kick off the training with an initial batch in the following way:</p>
            <br>
            <ol class="list-decimal pl-5 mt-4 text-gray-300">
                <li>Collect generator outputs</li>
                <li>Get human-annotation on these outputs</li>
                <li>Train the PRM by fine-tuning a base model on this human annotation</li>
            </ol>
            <br>
            <p>Then, they begin the iterative training procedure:</p>

            <ol class="list-decimal pl-5 mt-4 text-gray-300">
                <li>Collect generator outputs</li>
                <li>Determine the "most convincing wrong answers" using the latest version of the PRM. These are answers that are determined to be wrong, but contained CoTs that were able to trick the PRM.</li>
                <li>Surface the "most convincing wrong answers" to human annotators for labeling</li>
                <li>Include this newly labeled data in the training set</li>
                <li>Re-train the PRM on this data</li>
                <li>Repeat steps 1-5 iteratively until some level of convergence</li>
            </ol>
            <br>
            <p>An important detail to note is that the PRM also evaluates the final answer and treats it like any other step in the CoT.</p>

            <p>As part of their evaluation procedure, the authors develop an ORM (Outcome-supervised Reward Model) in addition to the PRM to compare performance. They find that while both methods outperform the base model with few-shot CoT and majority voting, the PRM considerably outperforms the ORM on (78.2% accuracy v/s 72.4% accuracy). This is insightful — the coherence of the CoT is important in leading it to a correct answer. It further underlines the idea that a purely outcome based model (like STaR) have areas for improvement, and encourages further work to train verifiers for the process, not just the outcome. I think there's a strong possibility that this training procedure, and RL based generator improvements are two of the fundamental building blocks of O1.</p>
            <br>
            <p>The authors of the STaR paper subsequently released Zelikman et al. 2024 <a href="#citation-8" style="color: #87CEEB;">[<span style="color: #87CEEB;">8</span>]</a> detailing the "Quiet-STaR" algorithm. This paper acknowledges and builds off of the following shortcomings of STaR identified by the authors:</p>

            <ul class="list-disc pl-5 mt-4 space-y-2 text-gray-300">
                <li>They trained and evaluated STaR on very narrow domains and only on question-answer type samples.</li>
                <li>Training on labeled samples of CoT is expensive and also off-policy.</li>
            </ul>
            <br>
            <p>To address these shortcomings, instead of training only on question-answer samples and basing the objective on the outcome, they formulate a non-myopic auto-regressive objective. Ie. They supervise their training procedure on the semantic correctness of the next N tokens predicted based on a rationale. They train the model on a more vast corpus of text, specifically OpenWebMath and Colossal Clean Crawled Corpus (C4).</p>
            <br>
            <img src="images/quiet_star.png" alt="Quiet-STaR Parallel Generation" class="w-1/2 mx-auto mb-2 mt-2">
            <p class="text-base text-gray-400 text-center">Parallel generation process in Quiet-STaR algorithm</p>
            <br>
            <p>The Quiet-STaR paper takes the idea of scaling inference-time compute further than the previously discussed algorithms. At each token in the input sequence, the model generates multiple rationales of length T to aid in the prediction of the next tokens in the sequence. This process obviously creates orders of magnitude of increased computational complexity which the authors reduce by generating these tokens in parallel for n tokens in the input sequence using an efficiently cached attention mask. helps visualize this process, and the open source code <insert link> details how this was implemented.</p>
            <br>
            <p>The objective of the training procedure is the sum of two separate loss functions. The first of these is the negative log likelihood loss of the predicted next-N tokens. The second of these is a formulation of the REINFORCE loss function. In order to construct the REINFORCE loss, they define the reward function as the difference between <span class="math inline">\(p^{talk}\)</span> and <span class="math inline">\(p^{-talk}\)</span> where <span class="math inline">\(p^{talk}\)</span> is the selected rationale's output distribution and <span class="math inline">\(p^{-talk}\)</span> is the average across rationales for that token. </p>
            <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
            <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
            <br>
            <p>This training procedure leads to improvements in the quality of outputs, with some notable improvements against the baseline being 8.1% v/s 5.9% on the GSM8K dataset, and 42.6% v/s 36.3% on the CQA dataset.</p>
            <br>
            <p>While these numbers are measurable improvements, they are far less impressive when normalizing this improvement by the increased computational complexity. This procedure also raises a few questions surrounding its practicality, a few of which were raised by the authors in the paper:</p>
            <br>
            <ul class="list-disc pl-5 mt-4 space-y-2 text-gray-300">
                <li>The authors note that the thought tokens (and therefore added inference time compute) helped with certain types of text, but with other domains led to no measurable improvements. This makes us question whether the added expenditure on test-time compute is worth it.</li>
                <li>They note that even within the same text, not all tokens benefit from having thought tokens leading up to them. This may lead to a waste of compute and realistically would be impractical to deploy in any production system</li>
            </ul>
            <br>
            <p>It's unlikely O1 is doing something like this on every single token, but I thought the formulation of the reinforce loss was an interesting way of incorporating RL and was worth noting.</p>
            <br>
            <p>Several other papers propose approaches along the lines of STaR. For example, Hosseini et al. 2024 <a href="#citation-9" style="color: #87CEEB;">[<span style="color: #87CEEB;">9</span>]</a> trains a verifier during the STaR training process to learn from wrong answers too. Instead of creating reverse engineered CoTs in the case of wrong answers, they use both the wrong and correct solutions to train a verifier using Direct Preference Optimization (DPO). At inference time, this verifier is used as a judge to select an answer from K proposals. </p>
            <br>
            <p>Most of the methods discussed so far are Verifier-based techniques to scale test-time compute. Ie, they use an auxiliary model that is trained to verify proposals sampled from a generator model. In August 2024, Google posted Snell et al. 2024 <a href="#citation-10" style="color: #87CEEB;">[<span style="color: #87CEEB;">10</span>]</a> that examines a number of verifier and non-verifier based methods of test-time compute scaling. Interestingly, they designed an experiment to study the effectiveness of different methods to scale test-time compute given a constant compute budget. This was a recent publication and details many different methods, so I won't go into too much detail on any one of them, but I will highlight a few things that stood out to me when I read through it.            </p>
            <br>
            <p>On a high level:</p>
            <ul class="list-disc pl-5 mt-4 space-y-2 text-gray-300">
                <li>The benefits of scaling methods vary across different types of problems.</li>
                <li>Scaling inference time compute helps more on more difficult problems as long as the base LLM contains the knowledge required to come up with an answer. The most challenging types of questions still benefit from additional pre-training data/compute. This result is loosely illustrated in the figure below.</li>
            </ul>
            <br>
            <img src="images/difficulty.png" alt="Relationship between problem difficulty and scaling methods" class="w-3/4 mx-auto mb-2 mt-2">
            <p class="text-base text-gray-400 text-center mb-4">Effectiveness of scaling methods across problem difficulty levels</p>
            <br>
            <p>Instead of using human annotation to train a PRM (like PRM800k), based on Wang et al. <a href="#citation-6" style="color: #87CEEB;">[<span style="color: #87CEEB;">6</span>]</a> they train and supervise one using estimates of per-step correctness obtained from running Monte Carlo rollouts from each step in the solution.</p>
            <br>
            <p>Below, I briefly describe the different methods they examined, and the results they each obtain.</p>
            <h3 class="text-xl font-semibold mt-4 mb-2">Verifier-based methods</h3>
            <img src="images/prm_search.png" alt="PRM Search Methods Comparison" class="w-3/4 mx-auto mb-4 mt-6">
            <p class="text-base text-gray-400 text-center mb-2">Different PRM-based search methods</p>
            <br>
            <ul class="list-disc pl-5 mt-2 space-y-2 text-gray-300">
                <li><strong>Beam search:</strong> explores multiple paths simultaneously, keeping the top K most promising candidates at each step and stepping from them at the next. It balances exploration and exploitation by maintaining a diverse set of high-scoring partial solutions. It performs better than all other verifier based methods on harder problems. You can think of beam search as lookahead search with K = 0 (this will make sense in one moment).</li>
                <li><strong>Lookahead search:</strong> extends beam search by performing rollouts of varying depths (K) to evaluate the potential of each candidate with the PRM scoring the final step of the rollout. It allows for deeper exploration of promising paths. This is more like the MCTS rollouts in AlphaZero, with the only difference being that the PRM is frozen here. This method blows up computationally, and there's a limit to how much they could scale in these experiments. It doesn't seem to outperform other methods as much as you'd think (at least based on these experiments).</li>
                <li><strong>Best of N weighted:</strong> This approach generates multiple solutions and groups them based on their final answers. The scores of solutions leading to the same correct answer are summed, and the answer with the highest total score is selected. This one's surprisingly good, and it keeps scaling well. Instead of a traditional best of N where we pick the highest rated final answer, they group together all solutions that lead to the same final answer, even if the steps or reasoning are different. Instead of just picking the single highest-scoring solution, they add up all the individual scores for the steps leading to each right answer. The final answer with the highest total score wins.</li>
                <li><strong>Majority voting:</strong> multiple solutions are generated, and the most common final answer is selected as the output. Performance hits a wall as the budget grows.</li>
            </ul>
            <br>
            <img src="images/verifier_results.png" alt="Verifier-based methods performance on MATH dataset" class="w-1/2 mx-auto mb-4 mt-4">
            <p class="text-base text-gray-400 text-center mb-2">Performance of verifier-based methods on MATH dataset vs. inference compute budget</p>
            <br>
            <h3 class="text-xl font-semibold mt-4 mb-2">Refining Proposal Distribution</h3>
            <p>Using the principle of self-revision, this approach allows language models to correct mistakes made in previous reasoning traces. They follow the training procedure from Qu et al. 2024 <a href="#citation-11" style="color: #87CEEB;">[<span style="color: #87CEEB;">11</span>]</a>. The main idea is that they formulate the training process as a multi-turn Markov Decision Process where at each turn the LLM can generate new tokens as well as edit tokens generated at previous steps. They divide compute to both sequential and parallel sampling to try and benefit from the local properties of sequential generation and the global properties of parallel sampling. This procedure is considerably different than the others discussed here, so I'll leave it for another time to explore its nuances. They find that on the MATH dataset this procedure can out-perform the search based methods at high compute budgets.</p>
            <br>
            <h3 class="text-xl font-semibold mt-4 mb-2">General comments:</h3>
            <p>It sort of feels like everyone, regardless of what specific approach they propose, believe in the three intuitions stated initially. Specifically, my takeaways are that inference-time compute scaling unlocks the ability for the model to interpolate better and training procedures for inference scaled models will have some sort of RL flavor to them. If these scaling laws stand the test of time, this would have broad implications across the stack. Inference compute will be more valuable than it previously was: it's typically easier to run model inference on a wide variety of GPUs (not just the top of the line NViDIA ones) which might (slightly) shake up the competitiveness of chip manufacturers. From a research perspective, resources will be split between methods to better scale pertaining and methods to better scale inference compute- even though the latter aims to scale test-time compute, experimentation and fine-tuning that leads to these types of systems will require training compute.</p>
            <br>
            <p>If there's one thing that the recent history of deep learning has taught me is that on largely general tasks, more simply crafted objectives produce astonishingly good solutions. I would expect that a similar scenario might play out with scaling inference time compute.</p>
            <br>
            <h3 class="text-xl font-semibold mt-4 mb-2">Bibliography</h3>
            <ol class="list-decimal pl-5 mt-4 space-y-2 text-gray-300">
                <li id="citation-1">Silver, David, et al. "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play." Science 362.6419 (2018): 1140-1144.</li>
                <li id="citation-2">Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." Advances in neural information processing systems 35 (2022): 24824-24837.</li>
                <li id="citation-3">Zhang, Hugh, and David C. Parkes. "Chain-of-thought reasoning is a policy improvement operator." arXiv preprint arXiv:2309.08589 (2023).</li>
                <li id="citation-4">Dutta, Subhabrata, et al. "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning." arXiv preprint arXiv:2402.18312 (2024).</li>
                <li id="citation-5">Lightman, Hunter, et al. "Let's verify step by step." arXiv preprint arXiv:2305.20050 (2023).</li>
                <li id="citation-6">Wang, P., et al. "Math-shepherd: Verify and reinforce llms step-by-step without human annotations." arXiv preprint arXiv:2308.13785 (2023).</li>
                <li id="citation-7">Zelikman, Eric, et al. "Star: Bootstrapping reasoning with reasoning." Advances in Neural Information Processing Systems 35 (2022): 15476-15488.</li>
                <li id="citation-8">Zelikman, Eric, et al. "Quiet-star: Language models can teach themselves to think before speaking." arXiv preprint arXiv:2403.09629 (2024).</li>
                <li id="citation-9">Hosseini, Arian, et al. "V-star: Training verifiers for self-taught reasoners." arXiv preprint arXiv:2402.06457 (2024).</li>   
                <li id="citation-10">Snell, Charlie, et al. "Scaling llm test-time compute optimally can be more effective than scaling model parameters." arXiv preprint arXiv:2408.03314 (2024).</li>
                <li id="citation-11">Qu, Yuxiao, et al. "Recursive introspection: Teaching language model agents how to self-improve." arXiv preprint arXiv:2407.18219 (2024).</li>
            </ol>
        </div>

    </div>

    
    

    <footer class="bg-gray-900 py-4">
    </footer>
</body>
</html>
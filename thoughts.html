<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Aryaman Pandya - Thoughts</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        body {
            font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
            font-weight: 400;
            background-color: #000000;
            color: #ffffff;
        }
        @media (max-width: 768px) {
            .container {
                padding-left: 16px;
                padding-right: 16px;
            }
        }
    </style>
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        sans: ['"Helvetica Neue"', 'Helvetica', 'Arial', 'sans-serif'],
                    },
                }
            }
        }
    </script>
    <link href="https://cdn.jsdelivr.net/npm/daisyui@3.7.3/dist/full.css" rel="stylesheet" type="text/css" />
</head>
<body class="font-sans leading-normal tracking-normal">
    <header class="text-center py-2">
        <h1 class="text-4xl font-normal mt-12 mb-4 text-white">Aryaman Pandya</h1>
    </header>

    <nav class="navbar bg-slate-800 opacity-70 text-white max-w-fit mx-auto rounded-xl mt-4 px-12">
        <div class="flex justify-center">
            <a href="index.html" class="btn btn-ghost text-lg normal-case font-light py-1">About</a>
            <a href="resume.html" class="btn btn-ghost text-lg normal-case font-light py-1">Resume</a>
            <a href="thoughts.html" class="btn btn-ghost text-lg normal-case font-light py-1">Thoughts</a>
        </div>
    </nav>

    <div class="container mx-auto px-4 sm:px-8 md:px-16 lg:px-32 xl:px-48 py-8 mt-6 mb-8">
        <p class="text-lg font-light text-center text-gray-300 mb-12 max-w-2xl mx-auto">
            An unstructured log of miscellaneous thoughts.
        </p>

        <div class="mb-12 bg-slate-800 rounded-xl p-6 max-w-lg mx-auto">
            <ul class="space-y-2 text-center">
                <li><a href="#thought1" class="text-blue-400 hover:text-blue-300 transition-colors duration-200">Quotes</a></li>
                <li><a href="#bookshelf" class="text-blue-400 hover:text-blue-300 transition-colors duration-200">Bookshelf</a></li>
                <li><a href="#thought2" class="text-blue-400 hover:text-blue-300 transition-colors duration-200">O1</a></li>
            </ul>
        </div>

        <div id="thought1" class="mb-12">
            <h3 class="text-2xl font-normal mb-4 text-white">Quotes</h3>
            <p class="text-gray-300">
               The following is a running list of quotes I found powerful when I came across them in books, podcasts, or other sources.
            </p>
            <ul class="list-disc pl-5 mt-4 space-y-4 text-gray-300">
                <li>
                    <p>"You can't know everything, but you should convince yourself you can know anything."</p>
                    <p class="text-sm text-gray-400 mt-1">- John Carmack</p>
                </li>
                <li>
                    <p>"We see an exponential curve but it's not because one thing is exponential, it's because we have hundreds of little sigmoids overlapped on each other that makes the general pattern look exponential"</p>
                    <p class="text-sm text-gray-400 mt-1">- John Carmack</p>
                </li>
                <li>
                    <p>"Up to a point it is better to let the snags [bugs] be there than to spend such time in design that there are none (how many decades would this course take?)."</p>
                    <p class="text-sm text-gray-400 mt-1">- Alan Turing</p>
                </li>
                <li>
                    <p>“Self-education is, I firmly believe, the only kind of education there is.”</p>
                    <p class="text-sm text-gray-400 mt-1">- Isaac Asimov</p>
                </li>
                <li>
                    <p>"After thousands of years of compounding scientific discovery and technological progress, we have figured out how to melt sand, add some impurities, arrange it with astonishing precision at extraordinarily tiny scale into computer chips, run energy through it, and end up with systems capable of creating increasingly capable artificial intelligence."</p>
                    <p class="text-sm text-gray-400 mt-1">- Sam Altman</p>
                </li>
                <li>
                    <p>"I wish I was weirder"</p>
                    <p class="text-sm text-gray-400 mt-1">- George Hotz</p>
                </li>
            </ul>
        </div>

        <div id="bookshelf" class="mb-12">
            <h3 class="text-2xl font-normal mb-4 text-white">Bookshelf</h3>
            <p class="text-gray-300 mb-4">
                A list of books on my bookshelf.
            </p>
            <ul class="list-disc pl-5 mt-4 space-y-4 text-gray-300">
                <li>
                    <p class="font-semibold">A Brief History of Time</p>
                    <p class="text-sm text-gray-400 mt-1">by Stephen Hawking</p>
                </li>
                <li>
                    <p class="font-semibold">Brief Answers to the Big Questions</p>
                    <p class="text-sm text-gray-400 mt-1">by Stephen Hawking</p>
                </li>
                <li>
                    <p class="font-semibold">QED: The Strange Theory of Light and Matter</p>
                    <p class="text-sm text-gray-400 mt-1">by Richard Feynman</p>
                </li>
                <li>
                    <p class="font-semibold">Six Not So Easy Pieces</p>
                    <p class="text-sm text-gray-400 mt-1">by Richard Feynman</p>
                </li>
                <li>
                    <p class="font-semibold">A New Kind of Science</p>
                    <p class="text-sm text-gray-400 mt-1">by Stephen Wolfram</p>
                </li>
                <li>
                    <p class="font-semibold">The Computer and the Brain</p>
                    <p class="text-sm text-gray-400 mt-1">by John Von Neumann</p>
                </li>
                <li>
                    <p class="font-semibold">The Annotated Turing</p>
                    <p class="text-sm text-gray-400 mt-1">by Charles Petzold</p>
                </li>
                <li>
                    <p class="font-semibold">The Art of Computer Programming Vol. 1: Fundamental Algorithms</p>
                    <p class="text-sm text-gray-400 mt-1">by Donald Knuth</p>
                </li>
                <li>
                    <p class="font-semibold">The Art of Computer Programming Vol. 2: Seminumerical Algorithms</p>
                    <p class="text-sm text-gray-400 mt-1">by Donald Knuth</p>
                </li>
                <li>
                    <p class="font-semibold">Centralized and Distributed Operating Systems</p>
                    <p class="text-sm text-gray-400 mt-1">by Gary J. Nutt</p>
                </li>
                <li>
                    <p class="font-semibold">Design Patterns: Elements of Reusable Object-Oriented Software</p>
                    <p class="text-sm text-gray-400 mt-1">by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides</p>
                </li>
                <li>
                    <p class="font-semibold">Clean Code: A Handbook of Agile Software Craftsmanship</p>
                    <p class="text-sm text-gray-400 mt-1">by Robert C. Martin</p>
                </li>
                <li>
                    <p class="font-semibold">Deep Learning with Python</p>
                    <p class="text-sm text-gray-400 mt-1">by François Chollet</p>
                </li>
                <li>
                    <p class="font-semibold">Building Machine Learning Powered Applications</p>
                    <p class="text-sm text-gray-400 mt-1">by Emmanuel Ameisein</p>
                </li>
                <li>
                    <p class="font-semibold">Probabilistic Robotics</p>
                    <p class="text-sm text-gray-400 mt-1">by Sebastian Thrun, Wolfram Burgard, and Dieter Fox</p>
                </li>
                <li>
                    <p class="font-semibold">Cracking the Coding Interview</p>
                    <p class="text-sm text-gray-400 mt-1">by Gayle Laakmann McDowell</p>
                </li>
                <li>
                    <p class="font-semibold">Hackers and Painters</p>
                    <p class="text-sm text-gray-400 mt-1">by Paul Graham</p>
                </li>
                <li>
                    <p class="font-semibold">Zero to One</p>
                    <p class="text-sm text-gray-400 mt-1">by Peter Thiel</p>
                </li>
                <li>
                    <p class="font-semibold">The Innovator's Dilemma</p>
                    <p class="text-sm text-gray-400 mt-1">by Clayton Christensen</p>
                </li>
                <li>
                    <p class="font-semibold">Obviously Awesome: How to Nail Product Positioning</p>
                    <p class="text-sm text-gray-400 mt-1">by April Dunford</p>
                </li>
                <li>
                    <p class="font-semibold">The General Theory of Employment, Interest and Money</p>
                    <p class="text-sm text-gray-400 mt-1">by J.M. Keynes</p>
                </li>
                <li>
                    <p class="font-semibold">Jobs</p>
                    <p class="text-sm text-gray-400 mt-1">by Walter Isaacson</p>
                </li>
                <li>
                    <p class="font-semibold">Elon Musk</p>
                    <p class="text-sm text-gray-400 mt-1">by Walter Isaacson</p>
                </li>
                <li>
                    <p class="font-semibold">The Idea Factory: Bell Labs and the Great Age of American Innovation</p>
                    <p class="text-sm text-gray-400 mt-1">by Jon Gertner</p>
                </li>
                <li>
                    <p class="font-semibold">Rafa</p>
                    <p class="text-sm text-gray-400 mt-1">by Rafael Nadal</p>
                </li>
                <li>
                    <p class="font-semibold">Open</p>
                    <p class="text-sm text-gray-400 mt-1">by Andre Agassi</p>
                </li>
                <li>
                    <p class="font-semibold">Meditations</p>
                    <p class="text-sm text-gray-400 mt-1">by Marcus Aurelius</p>
                </li>
                <li>
                    <p class="font-semibold">The Singularity Is Near</p>
                    <p class="text-sm text-gray-400 mt-1">by Ray Kurzweil</p>
                </li>
                <li>
                    <p class="font-semibold">Map and Territory</p>
                    <p class="text-sm text-gray-400 mt-1">by Eliezer Yudkowsky</p>
                </li>
                <li>
                    <p class="font-semibold">The Emotion Machine</p>
                    <p class="text-sm text-gray-400 mt-1">by Marvin Minsky</p>
                </li>
                <li>
                    <p class="font-semibold">The Society of Mind</p>
                    <p class="text-sm text-gray-400 mt-1">by Marvin Minsky</p>
                </li>
            </ul>
        </div>
    
        <div id="thought2" class="mb-12">
            <h3 class="text-3xl font-normal mb-4 text-white">O1: Modeling Chain of Thought Generation as a Reinforcement Learning Problem</h3>
            <h3 class="text-xl font-normal mb-4 text-slate-400">Aryaman Pandya</h3>
            <br>
            <p>In game-playing systems, neural networks perform considerably worse than versions of themself augmented with search. For example, a reproduction of AlphaZero showed that the network without Monte-Carlo Tree Search (MCTS) was able to achieve an chess ELO rating of 2500. This is high, but not by any means superhuman. The complete system with MCTS appended to the base model is able to achieve an ELO rating of 3500 (500 points higher than Magnus Carlson's).</p>
            <br>
            <p>During training and at test time, MCTS is able to search through potential futures, evaluate the outcomes of making different decisions, and select ones that maximize expected reward. In addition to test-time search, AlphaZero is trained with self-play, a Reinforcement Learning technique that enables the system to simulate games by playing against previous versions of itself. This synthetically generated experience reduces the dependency of the system on human generated training data and encourages deeper exploration of the solution space. This deep exploration can lead to the discovery of novel solutions in situations that are rarely encountered (see move 37, AlphaGo v/s Lee Sedol)</p>
            <br>
            <p>Similarly, embedding search-based mechanisms within the larger system should help large transformer-based language models produce better answers to hard, reasoning dependent problems. Hypothetically, these methods would enable the same kind of evaluation of future outcomes, allowing models to carefully answer questions while assessing the vast solutions pace.</p>
            <br>
            <p>Recently, OpenAI introduced the their "reasoning model", O1. O1 is said to have been trained using reinforcement learning and to "think" before answering questions. According to benchmark results reported by OpenAI, O1 indicates a step-function improvement to the capabilities of language models at solving difficult problems in domains like math, science and computer programming. Further, the publicly released O1-preview and O1-mini display step function improvements on the LMSYS leaderboard, which for the last year has primarily seen incremental improvements to its max score, thus proving its versatility in multiple domains. As we've come to expect, OpenAI withheld information about the training process they use as well as their datasets. They go as far as to protect the chains of thought generated by models in response to prompts. Therefore, what we can know about O1 is limited to the information they released in their blog posts, system card, and speculation from the community based on existing literature.</p>
            <br>
            <p>I thought it would be a good exercise for me to review surrounding literature to try and interpret what OpenAI might be doing here, and that's what this "article" is going to be about.</p>
            <br>
            <p>The success of O1 comes as the culmination of success of three intuitions:</p>
            <br>
            <ol class="list-decimal pl-5 mt-4 text-gray-300">
                <li>Scaling the amount of compute available to the model at inference time leads to better answers to difficult questions</li>
                <li>Chain of thought prompting elicits reasoning in large language models, and can be viewed as a policy improvement operator</li>
                <li>We can use reinforcement learning in conjunction with large reward models to build out a self-training process</li>
            </ol>
            <br>
            <p>In early 2022, OpenAI released "Chain of Thought Reasoning elicits reasoning in large language models". This paper demonstrated that including few-shot exemplars of chains of thought within prompts to the model can significantly increase the accuracy of its outputs. Interpretability research has shown that this is not just a surface level phenomenon, and that CoT prompting alters the weights of the attention layer, leading to different connections that produce better results. Few-shot CoT prompting has since been used as a reliable means of producing consistently higher quality outputs from language models across many domains. It is now standard practice to include samples that use CoT in datasets used to pre-train and fine-tune models.</p>
            <br>
            <p>While the description above would make it seem like this paper was the basis mainly for the first half of the second intuition listed above, I would argue that it fueled exploration along all of the other lines in that list. In this paper, the authors briefly mention that "additional compute can be allocated to more complex problems." While stated in passing (whether intentionally or not) this is a key insight we'll go into further detail on later on.</p>
            <br>
            <p>Shortly after the release of this paper, a Stanford group released "STaR: Bootstrapping Reasoning With Reasoning". This paper introduces the STaR (Self-taught Reasoner) fine-tuning process that combines CoT prompting with outcome supervision to produce an iteratively improving model. STaR has a generally simple training process:</p>
            <br>
            <ol class="list-decimal pl-5 mt-4 text-gray-300">
                <li>Using few shot CoT, produce responses from an LLM given a set of prompts</li>
                <li>Using automation, check for correctness of the outputs</li>
                <li>Append correct solutions (including the CoT that led to them) to a dataset that can be used to fine tune the model</li>
                <li>In situations where the answers are incorrect, prompt the LLM the correct answer and have it reverse engineer a CoT that would have led to this output</li>
                <li>Include these in the dataset used for finetuning</li>
                <li>Iterate</li>
            </ol>
            <br>
            <p>This method leverages the results from the OpenAI paper that CoT improves model outputs, and builds a process that allows for the model to iteratively teach itself to develop better chains of thought. This led to significant improvements over the benchmark results of the base model: _____insert results here____</p>

            <p>This paper also briefly formulates this the training objective as an approximation to the RL policy gradient objective. It doesn't, however, use this insight to guide the training process, or dive deeper into a mathematical analysis of this approximation.</p>
            <br>
            <p>STaR builds the intuition that more inference time compute can lead to better results, and also defines a self-play like training procedure. That being said, there were a few obvious shortcomings of this method:</p>
            <br>
            <ul class="list-disc pl-5 mt-4 text-gray-300">
                <li>As long as the final answer is correct, a sample is included in the iteratively generated fine-tuning dataset. This will include outputs where the CoT is erroneous but somehow the model produced the correct final answer.</li>
                <li>The authors try to avoid wasting data by generating reverse engineered CoTs, but these could produce an exaggerated version of the problem in bullet 1. There's no means in the methodology of this paper to verify that the forced CoTs contain coherent reasoning.</li>
                <li>There's value in learning from true negatives that this process leaves on the table.</li>
            </ul>
            <br>
            <p>The first two limitations mentioned above are due to the lack of process supervision. In outcome supervision, we use the correctness of the output of the model as the sole determinant of what samples to include in our dataset. In process supervision, we judge the correctness of not just the final result, but of each step that led to it. To address this, in  "Let's verify step-by-step", researchers from OpenAI train a Process-supervised Reward Model (PRM) by fine-tuning a base model on this human annotation</p>
            <br>
            <p>In this work, the authors focus solely on training a PRM, and not on fine-tuning the generator model. They do, however, state that "fine-tuning with RL is a natural next step." (Possibly foreshadowing O1?) Before kicking off the PRM training process, they fine-tune the generator to ensure that it produces CoTs in a consistent, line delimited format. They kick off the training with an initial batch in the following way:</p>
            <br>
            <ol class="list-decimal pl-5 mt-4 text-gray-300">
                <li>Collect generator outputs</li>
                <li>Get human-annotation on these outputs</li>
                <li>Train the process reward model by fine-tuning a base model on this human annotation</li>
            </ol>
            <br>
            <p>Then, they begin the iterative training procedure:</p>

            <ol class="list-decimal pl-5 mt-4 text-gray-300">
                <li>Collect generator outputs</li>
                <li>Determine the "most convincing wrong answers" using the latest version of the PRM. These are answers that are determined to be wrong, but contained CoTs that were able to trick the PRM.</li>
                <li>Surface the "most convincing wrong answers" to human annotators for labeling</li>
                <li>Include this newly labeled data in the training set</li>
                <li>Re-train the PRM on this data</li>
                <li>Repeat steps 1-5 iteratively until some level of convergence</li>
            </ol>
            <br>
            <p>An important detail to note is that the PRM also evaluates the final answer and treats it like any other step in the CoT.</p>

            <p>As part of their evaluation procedure, the authors develop an ORM in addition to the PRM to compare performance. They find that while both methods outperform the base model with few-shot CoT and majority voting, the PRM considerably outperforms the ORM on (78.2% accuracy v/s 72.4% accuracy). This is insightful — the coherence of the CoT is important in leading it to a correct answer. It further underlines the idea that a purely outcome based model (like STaR) have areas for improvement, and encourages further work to train verifiers for the process, not just the outcome. I think there's a strong possibility that this training procedure, and RL based generator improvements are two of the fundamental building blocks of O1.</p>
            <br>
            <p>The authors of the STaR paper subsequently released a paper detailing the "Quiet-STaR" algorithm. This paper acknowledges and builds off of the following shortcomings of STaR identified by the authors:</p>

            <ul class="list-disc pl-5 mt-4 text-gray-300">
                <li>They trained and evaluated STaR on very narrow domains and only on question-answer type samples.</li>
                <li>Training on labeled samples of CoT is expensive and also off-policy.</li>
            </ul>
            <br>
            <p>To address these shortcomings, instead of training only on question-answer samples and basing the objective on the outcome, they formulate a non-myopic auto-regressive objective. Ie. They supervise their training procedure on the semantic correctness of the next N tokens predicted based on a rationale. They train the model on a more vast corpus of text, specifically the _____insert datasets here______.</p>
            <br>
            <p>The Quiet-STaR paper takes the idea of scaling inference-time compute further than the previously discussed algorithms. At each token in the input sequence, the model generates multiple rationales of length T to aid in the prediction of the next tokens in the sequence. This process obviously creates orders of magnitude of increased computational complexity which the authors reduce by generating these tokens in parallel for n tokens in the input sequence using a cached attention mask. Image <insert number> helps visualize this process, and the open source code <insert link> details how this was implemented.</p>
            <br>
            <p>The objective of the training procedure is the sum of two separate loss functions. The first of these is the negative log likelihood loss of the predicted N tokens. The second of these is a formulation REINFORCE loss function. In order to construct the REINFORCE loss, they define the reward function as the difference between p^talk and p^-talk. This is then represented as:</p>
            <br>
            <p>This training procedure leads to improvements in the quality of outputs, with some notable improvements against the baseline being:</p>
            <br>
            <p>While these numbers are measurable improvements, they are far less impressive when normalizing this improvement by the increased computational complexity. This procedure also raises a few questions, a few of which were raised by the authors in the paper:</p>
            <br>
            <ul class="list-disc pl-5 mt-4 text-gray-300">
                <li>The authors note that the thought tokens (and therefore added inference time compute) helped with certain types of text, but with other domains led to no measurable improvements. Why is this procedure worth it then? Wouldn't this be a waste of both inference and training compute? Should we focus on narrower datasets for such fine-tuning?</li>
                <li>They note that even within the same text, not all tokens benefit from having thought tokens leading up to them. This may lead to a waste of compute and realistically would be impractical to deploy in any production system</li>
            </ul>
            <br>
            <p>It's unlikely O1 is doing something like this on every single token, but I thought the formulation of the reinforce loss was an interesting method that could be used in other training procedures as well.</p>
            <br>
            <p>Several other papers propose approaches along the lines of these previous works. For example, V-STaR trains a verifier during the STaR training process to learn from wrong answers too. Instead of creating reverse engineered CoTs in the case of wrong answers, we use both the wrong and correct solutions to train a verifier using DPO. At inference time, this verifier is used as a judge to select an answer from K proposals.</p>
            <br>
            </p>
        </div>
    </div>

    
    

    <footer class="bg-gray-900 py-4">
        <p class="text-center text-gray-400">&copy; 2024 Aryaman Pandya. All rights reserved.</p>
    </footer>
</body>
</html>